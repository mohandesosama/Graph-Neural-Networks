{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f642317-93ef-4114-8601-0e16cb8a9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class DataSprawlGNN(nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_dim=128):\n",
    "        super(DataSprawlGNN, self).__init__()\n",
    "        # Node feature processing\n",
    "        self.node_encoder = nn.Linear(num_node_features, hidden_dim)\n",
    "        \n",
    "        # Edge feature processing\n",
    "        self.edge_encoder = nn.Linear(num_edge_features, hidden_dim)\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Prediction heads\n",
    "        self.sensitive_classifier = nn.Linear(hidden_dim, 2)  # Binary classification for sensitive data\n",
    "        self.sprawl_predictor = nn.Linear(hidden_dim * 2, 1)  # Edge scoring for sprawl pathways\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        # Encode node features\n",
    "        x = F.relu(self.node_encoder(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Encode edge features\n",
    "        edge_embed = F.relu(self.edge_encoder(edge_attr))\n",
    "        \n",
    "        # Graph convolutions\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Sensitive data classification\n",
    "        sensitive_pred = self.sensitive_classifier(x)\n",
    "        \n",
    "        # Sprawl pathway prediction (edge scoring)\n",
    "        src, dst = edge_index\n",
    "        edge_scores = self.sprawl_predictor(torch.cat([x[src], x[dst]], dim=1))\n",
    "        \n",
    "        return sensitive_pred, edge_scores.squeeze()\n",
    "\n",
    "def load_and_preprocess_data(node_file, edge_file):\n",
    "    # Load node data\n",
    "    node_df = pd.read_csv(node_file)\n",
    "    \n",
    "    # Load edge data\n",
    "    edge_df = pd.read_csv(edge_file)\n",
    "    \n",
    "    # Preprocess node features\n",
    "    node_features = node_df.drop(['node_id', 'sensitive_label'], axis=1).values\n",
    "    node_labels = node_df['sensitive_label'].values\n",
    "    \n",
    "    # Preprocess edge features\n",
    "    edge_features = edge_df.drop(['source', 'target', 'sprawl_risk'], axis=1).values\n",
    "    edge_labels = edge_df['sprawl_risk'].values\n",
    "    \n",
    "    # Normalize features\n",
    "    node_scaler = StandardScaler()\n",
    "    node_features = node_scaler.fit_transform(node_features)\n",
    "    \n",
    "    edge_scaler = StandardScaler()\n",
    "    edge_features = edge_scaler.fit_transform(edge_features)\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = torch.tensor(edge_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    data = Data(\n",
    "        x=torch.tensor(node_features, dtype=torch.float),\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=torch.tensor(edge_features, dtype=torch.float),\n",
    "        y=torch.tensor(node_labels, dtype=torch.long),\n",
    "        edge_y=torch.tensor(edge_labels, dtype=torch.float)\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_model(data, epochs=100, batch_size=32):\n",
    "    # Split data into train/val/test\n",
    "    num_nodes = data.x.size(0)\n",
    "    num_edges = data.edge_index.size(1)\n",
    "    \n",
    "    # Node splits\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    # Random splits (60/20/20)\n",
    "    indices = torch.randperm(num_nodes)\n",
    "    train_mask[indices[:int(0.6*num_nodes)]] = True\n",
    "    val_mask[indices[int(0.6*num_nodes):int(0.8*num_nodes)]] = True\n",
    "    test_mask[indices[int(0.8*num_nodes):]] = True\n",
    "    \n",
    "    # Edge splits\n",
    "    edge_train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "    edge_val_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "    edge_test_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "    \n",
    "    edge_indices = torch.randperm(num_edges)\n",
    "    edge_train_mask[edge_indices[:int(0.6*num_edges)]] = True\n",
    "    edge_val_mask[edge_indices[int(0.6*num_edges):int(0.8*num_edges)]] = True\n",
    "    edge_test_mask[edge_indices[int(0.8*num_edges):]] = True\n",
    "    \n",
    "    # Add masks to data\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    data.edge_train_mask = edge_train_mask\n",
    "    data.edge_val_mask = edge_val_mask\n",
    "    data.edge_test_mask = edge_test_mask\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DataSprawlGNN(\n",
    "        num_node_features=data.x.size(1),\n",
    "        num_edge_features=data.edge_attr.size(1)\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss functions and optimizer\n",
    "    criterion_node = nn.CrossEntropyLoss()\n",
    "    criterion_edge = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        sensitive_pred, edge_scores = model(data)\n",
    "        \n",
    "        # Compute losses\n",
    "        node_loss = criterion_node(sensitive_pred[data.train_mask], data.y[data.train_mask])\n",
    "        edge_loss = criterion_edge(edge_scores[data.edge_train_mask], data.edge_y[data.edge_train_mask])\n",
    "        total_loss = node_loss + edge_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_sensitive_pred, val_edge_scores = model(data)\n",
    "            val_node_loss = criterion_node(val_sensitive_pred[data.val_mask], data.y[data.val_mask])\n",
    "            val_edge_loss = criterion_edge(val_edge_scores[data.edge_val_mask], data.edge_y[data.edge_val_mask])\n",
    "            val_total_loss = val_node_loss + val_edge_loss\n",
    "            \n",
    "        # Save losses for plotting\n",
    "        train_losses.append(total_loss.item())\n",
    "        val_losses.append(val_total_loss.item())\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_total_loss < best_val_loss:\n",
    "            best_val_loss = val_total_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} | Train Loss: {total_loss.item():.4f} | Val Loss: {val_total_loss.item():.4f}')\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sensitive_pred, edge_scores = model(data)\n",
    "        \n",
    "        # Node classification metrics\n",
    "        node_pred = sensitive_pred.argmax(dim=1)\n",
    "        node_acc = (node_pred[data.test_mask] == data.y[data.test_mask]).float().mean()\n",
    "        node_precision = precision_score(data.y[data.test_mask].cpu(), node_pred[data.test_mask].cpu())\n",
    "        node_recall = recall_score(data.y[data.test_mask].cpu(), node_pred[data.test_mask].cpu())\n",
    "        node_f1 = f1_score(data.y[data.test_mask].cpu(), node_pred[data.test_mask].cpu())\n",
    "        \n",
    "        # Edge prediction metrics\n",
    "        edge_pred = (torch.sigmoid(edge_scores) > 0.5).float()\n",
    "        edge_acc = (edge_pred[data.edge_test_mask] == data.edge_y[data.edge_test_mask]).float().mean()\n",
    "        edge_precision = precision_score(data.edge_y[data.edge_test_mask].cpu(), edge_pred[data.edge_test_mask].cpu())\n",
    "        edge_recall = recall_score(data.edge_y[data.edge_test_mask].cpu(), edge_pred[data.edge_test_mask].cpu())\n",
    "        edge_f1 = f1_score(data.edge_y[data.edge_test_mask].cpu(), edge_pred[data.edge_test_mask].cpu())\n",
    "        \n",
    "        print(\"\\nNode Classification Results:\")\n",
    "        print(f\"Accuracy: {node_acc:.4f} | Precision: {node_precision:.4f} | Recall: {node_recall:.4f} | F1: {node_f1:.4f}\")\n",
    "        \n",
    "        print(\"\\nEdge Prediction Results:\")\n",
    "        print(f\"Accuracy: {edge_acc:.4f} | Precision: {edge_precision:.4f} | Recall: {edge_recall:.4f} | F1: {edge_f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'node_metrics': {\n",
    "                'accuracy': node_acc.item(),\n",
    "                'precision': node_precision,\n",
    "                'recall': node_recall,\n",
    "                'f1': node_f1\n",
    "            },\n",
    "            'edge_metrics': {\n",
    "                'accuracy': edge_acc.item(),\n",
    "                'precision': edge_precision,\n",
    "                'recall': edge_recall,\n",
    "                'f1': edge_f1\n",
    "            }\n",
    "        }\n",
    "\n",
    "def generate_mitigation_recommendations(model, data, node_df, edge_df):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sensitive_pred, edge_scores = model(data)\n",
    "        \n",
    "        # Get top sensitive nodes\n",
    "        sensitive_probs = torch.softmax(sensitive_pred, dim=1)[:, 1]\n",
    "        top_sensitive_nodes = torch.topk(sensitive_probs, k=10)\n",
    "        \n",
    "        # Get top risky edges\n",
    "        edge_risks = torch.sigmoid(edge_scores)\n",
    "        top_risky_edges = torch.topk(edge_risks, k=10)\n",
    "        \n",
    "        print(\"\\nTop Sensitive Data Assets:\")\n",
    "        for i, (node_idx, score) in enumerate(zip(top_sensitive_nodes.indices, top_sensitive_nodes.values)):\n",
    "            node_info = node_df.iloc[node_idx.item()]\n",
    "            print(f\"{i+1}. Node {node_idx.item()} (Score: {score:.4f})\")\n",
    "            print(f\"   Type: {node_info['type']}, Size: {node_info['size']}, Last Accessed: {node_info['last_accessed']}\")\n",
    "        \n",
    "        print(\"\\nTop Risky Sprawl Pathways:\")\n",
    "        for i, (edge_idx, score) in enumerate(zip(top_risky_edges.indices, top_risky_edges.values)):\n",
    "            edge_info = edge_df.iloc[edge_idx.item()]\n",
    "            print(f\"{i+1}. Edge {edge_idx.item()} (Score: {score:.4f})\")\n",
    "            print(f\"   Source: {edge_info['source']} -> Target: {edge_info['target']}\")\n",
    "            print(f\"   Access Frequency: {edge_info['access_frequency']}, Sharing Level: {edge_info['sharing_level']}\")\n",
    "        \n",
    "        return {\n",
    "            'sensitive_nodes': [(idx.item(), score.item()) for idx, score in zip(top_sensitive_nodes.indices, top_sensitive_nodes.values)],\n",
    "            'risky_edges': [(idx.item(), score.item()) for idx, score in zip(top_risky_edges.indices, top_risky_edges.values)]\n",
    "        }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Wait for dataset files (you'll provide these)\n",
    "    print(\"Please provide the paths to your node and edge CSV files when ready.\")\n",
    "    print(\"Expected format:\")\n",
    "    print(\"Node CSV: node_id, [features...], sensitive_label\")\n",
    "    print(\"Edge CSV: source, target, [features...], sprawl_risk\")\n",
    "    \n",
    "    # Example usage (uncomment when files are available):\n",
    "    \"\"\"\n",
    "    node_file = \"nodes.csv\"\n",
    "    edge_file = \"edges.csv\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess_data(node_file, edge_file)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Load the node and edge DataFrames for recommendations\n",
    "    node_df = pd.read_csv(node_file)\n",
    "    edge_df = pd.read_csv(edge_file)\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(data)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(model, data)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = generate_mitigation_recommendations(model, data, node_df, edge_df)\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
